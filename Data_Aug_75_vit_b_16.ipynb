{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JeAD6OIuTVx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vit_b_16\n",
        "from torch.optim import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import kagglehub\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import ssl\n",
        "from timm import create_model\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WWMO93OZJHY",
        "outputId": "ef030465-c895-4caf-f8dd-120df348678d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/immada/casia-fasd?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.05G/2.05G [00:12<00:00, 174MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/immada/casia-fasd/versions/1\n"
          ]
        }
      ],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"immada/casia-fasd\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8W0P9P2ugPA",
        "outputId": "6275bd97-c766-4cd8-8ea9-1de2a3343151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verificando o diretório: /root/.cache/kagglehub/datasets/immada/casia-fasd/versions/1/casia-fasd/train/live\n",
            "Verificando o diretório: /root/.cache/kagglehub/datasets/immada/casia-fasd/versions/1/casia-fasd/train/spoof\n",
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n",
            "Verificando o diretório: /root/.cache/kagglehub/datasets/immada/casia-fasd/versions/1/casia-fasd/test/live\n",
            "Verificando o diretório: /root/.cache/kagglehub/datasets/immada/casia-fasd/versions/1/casia-fasd/test/spoof\n",
            "Tamanho do Dataset de Treino: 38022\n",
            "Tamanho do Dataset de Teste: 65786\n",
            "Distribuição no Treino: Counter({0: 19011, 1: 19011})\n",
            "Distribuição no Teste: Counter({0: 55658, 1: 10128})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Dataset personalizado para o CASIA-FASD com balanceamento opcional\n",
        "class CASIA_FASD_Dataset(Dataset):\n",
        "    def __init__(self, base_dir, dataset_type=\"train\", transform=None):\n",
        "        self.base_dir = base_dir\n",
        "        self.dataset_type = dataset_type\n",
        "        self.transform = transform\n",
        "\n",
        "        self.live_paths = []\n",
        "        self.spoof_paths = []\n",
        "\n",
        "        for label_dir in [\"live\", \"spoof\"]:\n",
        "            label = 1 if label_dir == \"live\" else 0\n",
        "            label_path = os.path.join(base_dir, dataset_type, label_dir)\n",
        "            print(f\"Verificando o diretório: {label_path}\")\n",
        "            for root, _, files in os.walk(label_path):\n",
        "                for file in files:\n",
        "                    if file.endswith('.png'):\n",
        "                        full_path = os.path.join(root, file)\n",
        "                        if label == 1:\n",
        "                            self.live_paths.append((full_path, label))\n",
        "                        else:\n",
        "                            self.spoof_paths.append((full_path, label))\n",
        "\n",
        "        self.refresh()\n",
        "\n",
        "    def refresh(self):\n",
        "      if self.dataset_type == \"train\":\n",
        "        # Balanceamento das classes por undersampling\n",
        "        min_len = min(len(self.live_paths), len(self.spoof_paths))\n",
        "        print(f\"[REFRESH] Balanceando para {min_len} imagens por classe nesta época\")\n",
        "\n",
        "        sampled_live = random.sample(self.live_paths, min_len)\n",
        "        sampled_spoof = random.sample(self.spoof_paths, min_len)\n",
        "\n",
        "        balanced_data = sampled_live + sampled_spoof\n",
        "        random.shuffle(balanced_data)\n",
        "\n",
        "        self.image_paths, self.labels = zip(*balanced_data)\n",
        "        self.image_paths = list(self.image_paths)\n",
        "        self.labels = list(self.labels)\n",
        "      else:\n",
        "        # Não balanceia os dados para validação ou teste\n",
        "        all_data = self.live_paths + self.spoof_paths\n",
        "        self.image_paths, self.labels = zip(*all_data)\n",
        "        self.image_paths = list(self.image_paths)\n",
        "        self.labels = list(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        return image, label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"CASIA_FASD_Dataset(dataset_type='{self.dataset_type}', num_samples={len(self)})\"\n",
        "\n",
        "# Caminho base do dataset\n",
        "base_dir = r\"/root/.cache/kagglehub/datasets/immada/casia-fasd/versions/1/casia-fasd\"\n",
        "\n",
        "# Transformações dos frames\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomApply([\n",
        "        transforms.RandomHorizontalFlip(p=1.0),\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.RandomAffine(degrees=15, translate=(0.05, 0.05))\n",
        "    ], p=0.75),  # 75% das imagens sofrerão essas mudanças\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_normalize = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Criar Datasets e DataLoaders\n",
        "train_dataset = CASIA_FASD_Dataset(base_dir, dataset_type=\"train\", transform=transform)\n",
        "test_dataset = CASIA_FASD_Dataset(base_dir, dataset_type=\"test\", transform=transform_normalize)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, num_workers=20, pin_memory=True, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, num_workers=20, pin_memory=True, shuffle=False)\n",
        "\n",
        "# Exibir os tamanhos dos datasets\n",
        "print(f\"Tamanho do Dataset de Treino: {len(train_dataset)}\")\n",
        "print(f\"Tamanho do Dataset de Teste: {len(test_dataset)}\")\n",
        "\n",
        "train_labels = train_dataset.labels\n",
        "test_labels = test_dataset.labels\n",
        "\n",
        "print(\"Distribuição no Treino:\", Counter(train_labels))\n",
        "print(\"Distribuição no Teste:\", Counter(test_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1180ba884d534a708cc9d57e0ce3a0a8",
            "9f41b3d45b49454fa1d1877359d4782d",
            "122113b31a2244af80d0313a0f87dbff",
            "205dd71b0b0e4e9fa3e96b29df50cb14",
            "de277308dec747418861c4b7aec78f56",
            "1ff26c26334548c1a9e3a078aed055e6",
            "9dd1ad7d15ae45b1b971526c9b5e2504",
            "b629b18f3b664cf796e54011863c0bd2",
            "81caa55787694f3581b3ea7e9e04413c",
            "5780ceacd2034dab846fb580219b4cab",
            "56827f191f2443c8927bfed92aa92773"
          ]
        },
        "id": "jmKNbN9uu2re",
        "outputId": "7c891506-8a67-4ad1-d6ba-ae458691a1e1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1180ba884d534a708cc9d57e0ce3a0a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 298/298 [04:03<00:00,  1.22it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 298/298 [04:02<00:00,  1.23it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 298/298 [04:02<00:00,  1.23it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 298/298 [04:01<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 298/298 [04:02<00:00,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliação após Epoch 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 1.0000\n",
            "Precisão: 1.0000\n",
            "Recall: 0.9999\n",
            "F1-score: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.9993\n",
            "Precisão: 0.9985\n",
            "Recall: 0.9970\n",
            "F1-score: 0.9978\n",
            "\n",
            "Novo melhor modelo salvo com acurácia: 0.9993\n",
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6/20:   0%|          | 0/298 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 6/20: 100%|██████████| 298/298 [04:01<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 298/298 [04:01<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 298/298 [04:01<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 298/298 [04:01<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 298/298 [04:02<00:00,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliação após Epoch 10:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 1.0000\n",
            "Precisão: 1.0000\n",
            "Recall: 1.0000\n",
            "F1-score: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.9994\n",
            "Precisão: 0.9993\n",
            "Recall: 0.9969\n",
            "F1-score: 0.9981\n",
            "\n",
            "Novo melhor modelo salvo com acurácia: 0.9994\n",
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 11/20:   0%|          | 0/298 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 11/20: 100%|██████████| 298/298 [04:01<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 298/298 [04:01<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 298/298 [04:01<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 298/298 [04:01<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 298/298 [04:00<00:00,  1.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliação após Epoch 15:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 1.0000\n",
            "Precisão: 1.0000\n",
            "Recall: 1.0000\n",
            "F1-score: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.9955\n",
            "Precisão: 0.9745\n",
            "Recall: 0.9967\n",
            "F1-score: 0.9855\n",
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 16/20:   0%|          | 0/298 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 16/20: 100%|██████████| 298/298 [04:00<00:00,  1.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 298/298 [04:01<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 298/298 [04:01<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 298/298 [04:01<00:00,  1.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[REFRESH] Balanceando para 19011 imagens por classe nesta época\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 298/298 [04:01<00:00,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Avaliação após Epoch 20:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 1.0000\n",
            "Precisão: 1.0000\n",
            "Recall: 1.0000\n",
            "F1-score: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.9984\n",
            "Precisão: 0.9989\n",
            "Recall: 0.9907\n",
            "F1-score: 0.9948\n",
            "\n",
            "Avaliação Final no Conjunto de Teste:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 20 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia: 0.9984\n",
            "Precisão: 0.9989\n",
            "Recall: 0.9907\n",
            "F1-score: 0.9948\n",
            "AUC-ROC: 1.0000\n",
            "MCC: 0.9939\n",
            "Matriz de Confusão:\n",
            "[[55647    11]\n",
            " [   94 10034]]\n",
            "Specificidade: 0.9998\n",
            "\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       spoof       1.00      1.00      1.00     55658\n",
            "        live       1.00      0.99      0.99     10128\n",
            "\n",
            "    accuracy                           1.00     65786\n",
            "   macro avg       1.00      1.00      1.00     65786\n",
            "weighted avg       1.00      1.00      1.00     65786\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9984039157267504"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score,\n",
        "    matthews_corrcoef\n",
        ")\n",
        "import ssl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from timm import create_model\n",
        "\n",
        "# Configuração do dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Modelo Vision Transformer (ViT) para classificação binária\n",
        "model = create_model('vit_base_patch16_224', pretrained=True)\n",
        "model.head = nn.Linear(model.head.in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# Função de custo com pesos\n",
        "weights = torch.tensor([1.0, 1.0], dtype=torch.float).to(device)  # Ajuste os pesos conforme necessário\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "optimizer = Adam(model.parameters(), lr=1e-6)\n",
        "\n",
        "# Avaliação do modelo\n",
        "def evaluate_model(model, dataloader, device, full_eval=False):\n",
        "    model.eval()\n",
        "    y_true, y_pred, y_probs = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "            y_probs.extend(probs[:, 1].cpu().numpy())\n",
        "\n",
        "    y_true, y_pred, y_probs = np.array(y_true), np.array(y_pred), np.array(y_probs)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    print(f\"Acurácia: {acc:.4f}\\nPrecisão: {prec:.4f}\\nRecall: {rec:.4f}\\nF1-score: {f1:.4f}\")\n",
        "\n",
        "    if full_eval:\n",
        "        auc = roc_auc_score(y_true, y_probs)\n",
        "        mcc = matthews_corrcoef(y_true, y_pred)\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        print(f\"AUC-ROC: {auc:.4f}\\nMCC: {mcc:.4f}\\nMatriz de Confusão:\\n{cm}\")\n",
        "\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        specificity = tn / (tn + fp)\n",
        "        print(f\"Specificidade: {specificity:.4f}\\n\")\n",
        "        print(\"Relatório de Classificação:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=[\"spoof\", \"live\"]))\n",
        "\n",
        "    model.train()\n",
        "    return acc\n",
        "\n",
        "# Treinamento com salvamento do melhor modelo\n",
        "best_acc = 0\n",
        "best_model_path = \"melhor_modelo_vit.pth\"\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_dataset.refresh()\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"\\nAvaliação após Epoch {epoch+1}:\")\n",
        "        train_acc = evaluate_model(model, train_loader, device, full_eval=False)\n",
        "\n",
        "        val_acc = evaluate_model(model, test_loader, device, full_eval=False)\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"\\nNovo melhor modelo salvo com acurácia: {best_acc:.4f}\")\n",
        "\n",
        "print(\"\\nAvaliação Final no Conjunto de Teste:\")\n",
        "#model.load_state_dict(torch.load(best_model_path))\n",
        "evaluate_model(model, test_loader, device, full_eval=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1180ba884d534a708cc9d57e0ce3a0a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f41b3d45b49454fa1d1877359d4782d",
              "IPY_MODEL_122113b31a2244af80d0313a0f87dbff",
              "IPY_MODEL_205dd71b0b0e4e9fa3e96b29df50cb14"
            ],
            "layout": "IPY_MODEL_de277308dec747418861c4b7aec78f56"
          }
        },
        "9f41b3d45b49454fa1d1877359d4782d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ff26c26334548c1a9e3a078aed055e6",
            "placeholder": "​",
            "style": "IPY_MODEL_9dd1ad7d15ae45b1b971526c9b5e2504",
            "value": "model.safetensors: 100%"
          }
        },
        "122113b31a2244af80d0313a0f87dbff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b629b18f3b664cf796e54011863c0bd2",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81caa55787694f3581b3ea7e9e04413c",
            "value": 346284714
          }
        },
        "205dd71b0b0e4e9fa3e96b29df50cb14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5780ceacd2034dab846fb580219b4cab",
            "placeholder": "​",
            "style": "IPY_MODEL_56827f191f2443c8927bfed92aa92773",
            "value": " 346M/346M [00:01&lt;00:00, 254MB/s]"
          }
        },
        "de277308dec747418861c4b7aec78f56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ff26c26334548c1a9e3a078aed055e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dd1ad7d15ae45b1b971526c9b5e2504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b629b18f3b664cf796e54011863c0bd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81caa55787694f3581b3ea7e9e04413c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5780ceacd2034dab846fb580219b4cab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56827f191f2443c8927bfed92aa92773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}